% !TeX root = ../main.tex

\chapter{绪论}

在大数据时代，随着数据量的迅速增加，我们往往需要更低的时间复杂度算法来完成任务。此外，
在一些实际场景中，我们的数据是动态的，即数据集合会随着时间的推移而发生变化。显然，如果数据量极其庞大，
当训练数据动态变化时，重新训练整个模型的代价是不可行的。

为了解决这些问题，一个很自然的想法是构建一个小规模的训练集，以便我们可以在小规模集合而不是所有数据上运行现有的算法。
核心集（coreset）\cite{DBLP:journals/corr/abs-2011-09384}，最初出现于计算几何\cite{DBLP:journals/jacm/AgarwalHV04}研究中的概念，已成为许多大规模机器学习问题\cite{DBLP:journals/corr/BravermanFL16}\cite{DBLP:conf/gi/MunteanuSSW19}中广泛使用的数据压缩技术。作为一种简洁的
数据压缩技术，核心集还具有一些优秀的性质。例如，coreset通常是可组合的，因此可以应用于分布式计算环境之中\cite{DBLP:conf/pods/IndykMMM14}。
此外，它往往能在流算法\cite{DBLP:conf/stoc/Har-PeledM04}和包含数据增删的动态算法\cite{DBLP:conf/esa/HenzingerK20}中取得较好的表现。

然而，现有的coreset算法仍然有较大的改进空间。一个主要的瓶颈是，多数coreset算法容易受到异常点（离群点）的影响
而导致性能下降，而现实数据集中往往有许多噪声。许多现有研究也表明，只需在训练样本中加入少数攻击点，就可能导致原模型的效果显著下降。\cite{DBLP:journals/pr/BiggioR18}
因此，如何设计一个鲁棒性更好的核心集算法是一个重要的研究方向。

为了简要说明现有的核心集方法对离群值的敏感性，我们可以以一种基于采样的coreset算法为例\cite{DBLP:conf/stoc/FeldmanL11}。这个算法需要对每个数据点计算一个“敏感度，
该敏感度衡量了该数据项对整个数据集的重要程度；然而，它往往会给远离数据多数部分的点分配高敏感度，也就是说，离群点往往会有更高的敏感度，
从而相比正常的点拥有更高的进入核心集的概率。这种现象在实际应用中可能会导致核心集的性能下降，我们希望核心集中包含更多的正常点。
现有的一些鲁棒 核心集 构建方法通常依赖于简单的均匀采样\cite{DBLP:conf/stoc/FeldmanL11}\cite{DBLP:conf/focs/HuangJLW18}，并且仅在离群值的数量是输入规模的常数因子的情况下才有效。


\section{本文贡献}

在本文中，我们使用了一个统一的完全动态鲁棒 核心集框架\cite{Wang2021RobustAF}，适用于一类称为连续且有界（CnB）学习的优化问题。
这类学习问题涵盖了机器学习中的一类广泛优化目标。
简单来说，“CnB”要求优化目标是一个连续函数（例如，平滑或Lipschit连续），同时解决方案被限制在一个有界区域内。
我们强调这种“有界”假设在实际机器学习场景中是非常自然的。
为了阐明这一点，可以考虑一个迭代算法（例如常见的梯度下降或EM算法）来优化某个目标；除了最初的几轮，问题的解总是限制在一个局部区域内。
此外，在动态情况中将解决方案范围限定在有界区域也是合理的，因为单次更新（插入或删除）数据不太可能显著改变解。

首先，我们假设存在一个用于给定 CnB 优化目标的普通 核心集 构建方法 A（不考虑离群值）。
算法思路是将输入数据分为两部分：“疑似正常点”和“疑似离群点”，其中这两部分的大小比例是一个参数 λ。对于“疑似正常点”，
我们运行方法 A（作为一个黑箱）；对于“疑似离群点”，我们直接随机均匀地抽取一个小样本；
这两部分共同构成一个鲁棒的核心集。
这个框架也可以在动态数据情况下高效地在“merge-and-reduce”框架中实现（尽管原始的合并和减少框架并未设计用于包含离群值的情况）\cite{DBLP:conf/stoc/Har-PeledM04,DBLP:conf/esa/HenzingerK20}。
该框架的一个有趣特性是，如果动态数据中离群值的比例发生变化，我们可以轻松调整参数z以动态更新我们的核心集。

本文的另一贡献是，对于“疑似离群点”的随机采样，证明其采样复杂度与参数空间的加倍维度有关，相比于之前的工作，我们的结果在参数空间有较好性质时，能得到更优的结果。
先前的结果中，对于“疑似离群点”的随机采样复杂度通常与数据空间的VC维度有关，而VC维度在实际应用中可能过大，甚至难以计算。




\section{符号定义}


我们介绍本文中使用的几个重要符号。假设 $\mathcal{P}$ 是参数空间。设 $X$ 为输入数据集，包含 $n$ 个项，位于度量空间 $X$ 中，每个 $x \in X$ 有一个权重 $w(x) \geq 0$。此外，我们用 $(X, z)$ 表示一个具有 $z$ 个离群值的给定实例 $X$。

我们使用 $| \cdot |$ 和 $[[ \cdot ]]$ 分别表示给定数据集的数据项数量和总权重。我们考虑的机器学习问题，其损失函数是 $X$ 上代价的加权和，即：
\begin{equation}
f(\theta, X) := \sum_{x \in X} w(x) f(\theta, x).
\label{eq:loss}
\end{equation}

其中，$f(\theta, x)$ 是参数向量 $\theta \in P$ 对应的由 $x$ 贡献的非负代价。目标是找到一个合适的 $\theta$ 使得损失函数 $f(\theta, X)$ 最小化。通常我们假设 $X$ 中的每个 $x$ 都有单位权重（即，$w(x) = 1$），并且很容易将我们的方法扩展到加权情况。

给定 $X$ 中预定数量的离群值 $z \in \mathbb{Z}^+$（对于加权情况，“$z$”指的是离群值的总权重），我们定义“鲁棒”损失函数：
\begin{equation}
f_z(\theta, X) := \min_{O \subset X, [[O]] = z} f(\theta, X \setminus O).
\label{eq:robust loss}
\end{equation}

实际上，上述定义~\eqref{eq:robust loss}来自于“修剪”思想，已被广泛应用于鲁棒优化问题\cite{DBLP:books/wi/RousseeuwL87}。

接下来，我们给出CnB问题的形式定义。函数 $g: \mathcal{P} \rightarrow \mathbb{R}$ 是 $\alpha$-Lipschitz 连续的，
如果对于任意 $\theta_1, \theta_2 \in P$，有 $|g(\theta_1) - g(\theta_2)| \leq \alpha \|\Delta \theta\|$，
其中 $\Delta \theta = \theta_1 - \theta_2$ 且 $\|\cdot\|$ 是 $P$ 中某个指定的范数。
后文中为了方便，我们往往记为$|g(\theta_1) - g(\theta_2)| \leq \alpha \|\increment \theta\| = \xi(\increment \theta)$，其中 $\xi(\cdot)$ 是一个线性函数。


\begin{definition}[CnB 问题]
  令 $\alpha > 0$，$\ell > 0$，并且 $\tilde{\theta} \in \mathcal{P}$。用 $\mathbb{B}(\tilde{\theta}, \ell)$ 表示在
  参数空间 $\mathcal{P}$ 中以 $\tilde{\theta}$ 为中心、半径为 $\ell$ 的球。

如果损失函数~\eqref{eq:loss} 满足以下条件，则称其为具有参数 $(\alpha, \ell, \tilde{\theta})$ 的 CnB 学习问题：

（i）对于任何 $x \in X$，损失函数 $f(\cdot, x)$ 是 $\alpha$-Lipschitz 连续的。

（ii）$\theta$ 始终限制在 $\mathbb{B}(\tilde{\theta}, \ell)$ 内。
\label{def:CnB}
\end{definition}

我们定义CnB问题中的核心集如下：

\begin{definition}[$\varepsilon$-coreset]
  令 $\varepsilon > 0$。给定一个数据集 $X \subseteq \mathcal{X}$ 
  和损失函数 $f(\theta, X)$，如果加权集合 $C \subseteq \mathcal{X}$ 满足对于
  任意 $\theta \in \mathbb{B}(\tilde{\theta}, \ell)$，都有
  \begin{equation*}
  |f(\theta, C) - f(\theta, X)| \leq \varepsilon f(\theta, X),
  \end{equation*}
则称 $C$ 是 $X$ 的一个 $\varepsilon$-coreset。
\label{def:coreset}
\end{definition}


如果 $C$ 是 $X$ 的 $\varepsilon$-coreset，我们可以在 $C$ 上运行现有的优化算法，从而获得一个近似解。显然，我们期望 $C$ 的大小尽可能小。
根据定义\ref{def:coreset}，我们还定义了相应的鲁棒核心集。


\begin{definition}[鲁棒核心集]
  令 $\varepsilon > 0$，$\beta \in [0, 1]$。给定数据集 $X \subseteq \mathcal{X}$ 
  和损失函数 $f(\theta, x)$，如果加权集合 $C \subseteq \mathcal{X}$ 满足对于任意 $\theta \in \mathbb{B}(\tilde{\theta}, \ell)$，都有
  \begin{equation*}
  (1 - \varepsilon) f_{(1+\beta)z}(\theta, X) \leq f_z(\theta, C) \leq (1 + \varepsilon) f_{(1-\beta)z}(\theta, X),
  \end{equation*}

则称 $C$ 是 $X$ 的一个 $(\beta, \varepsilon)$-鲁棒 核心集。
\label{def:robust coreset}
\end{definition}


大致来说，如果我们在 $C$ 上获得一个近似解 $\theta' \in \mathcal{P}$，那么该解
在原始输入数据 $X$ 上的效果也能得到保持。参数 $\beta$ 表示在使用 $\theta'$ 作为 $X$ 的解时，离群值数量的误
差。如果我们设定 $\beta = 0$，这意味着我们允许离群值数量没有误差。

本文对于采样复杂度的讨论中常常涉及VC维度和加倍维度的概念。在此我们简单介绍这两个维度以及相关概念。

\begin{definition}[f-诱导的范围空间]
  假设 $\mathcal{X}$ 是一个任意度量空间。给定 $X$ 上的损失函数 $f(\theta, X)$ 如公式~\eqref{eq:loss} 所示，我们定义
  \begin{equation}
  \mathcal{R} = \left\{ \{ x \in \mathcal{X} : f(\theta, x) \leq r \} \mid \forall r \geq 0, \forall \theta \in \mathcal{P} \right\},
  \end{equation}

则 $(\mathcal{X}, \mathcal{R})$ 称为 $f$-诱导的范围空间。
每个 $R \in \mathcal{R}$ 称为 $\mathcal{X}$ 的一个范围。
\end{definition}

\begin{definition}[shattered set]
  对于一个f-诱导的范围空间$(\mathcal{X},\mathcal{R})$，
  如果一个集合$P\subset \mathcal{X}$，对应的$\mathcal{R}_{|P}=\left\{P\cap Q|Q\in \mathcal{R}\right\}$满足：
  \begin{equation*}
    |\mathcal{R}_{|P}|=2^{|P|}
  \end{equation*}
  那么我们称为集合P是“shattered”的集合。
\end{definition}
% 引用
\begin{definition}[VC dimension]
  对于一个f-诱导的范围空间$(\mathcal{X},\mathcal{R})$，其vcdim指$\max\left\{k\left.\right|\forall |P|=k,P\subset \mathcal{X},|\mathcal{R}_{|P}|=2^{|P|}\right\}$。
\end{definition}

简单来说，vcdim衡量了$f(\cdot)$，$\mathcal{X}$的性质，在本文的记号下还有参数$\theta$的性质。
以一个简单的机器学习问题为例，$f(\theta,\mathbf{x})=ax+by$，$\mathcal{X}=\mathbb{R}^2$，$\theta=(a,b)\in \mathbb{R}^2$衡量了一个平面上线性分类器的分类能力。

本文中另一个概念，加倍维度\cite{DBLP:journals/talg/ChanGMZ16}，相比而言简洁一些。它关注一个度量空间的性质。

\begin{definition}[Doubling dimension]
  给定一个度量空间$\mathcal{P}$，其加倍维度，ddim，表示对于一个球$\mathbb{B}(r)$，最少能用多少个半径为$r/2$的球
  $\mathbb{B}(r/2)$所覆盖。
\end{definition}

可以看到，加倍维度的定义中没有引入任何形式的损失函数$f(\cdot)$，这在机器学习问题中是一个优秀的性质。本文后续关于加倍维度的讨论均指参数空间$\mathcal{P}$
的加倍维度。一个具有加倍维度d的空间往往也被称为加倍空间。

\textbf{本文后续的内容组织如下：}在第二章中，我们证明了一些关于均匀采样的性质，这些性质在后续的核心集构建中是非常重要的。
在第三章中，我们介绍了一个适用于CnB问题的完全动态的鲁棒核心集框架。
在第四章中，我们给出了两种黑盒的核心集方法，并证明在CnB问题中，其采样复杂度与加倍维度有关。在第五章中，我们实验验证核心集框架的加速效果。